{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Millimono/Asignement3_Autonme2025_cas1.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuGZxJklFqEo",
        "outputId": "bf64966a-d3b5-4aed-bcf1-48f53ca7867f"
      },
      "id": "yuGZxJklFqEo",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Asignement3_Autonme2025_cas1'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 23 (delta 6), reused 23 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (23/23), 19.04 KiB | 2.72 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Asignement3_Autonme2025_cas1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoZac3lHFxFh",
        "outputId": "06f666c9-cd12-4adb-971b-62296b4dd3c1"
      },
      "id": "KoZac3lHFxFh",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'Asignement3_Autonme2025_cas1'\n",
            "/content/Asignement3_Autonme2025_cas1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "595854f8",
      "metadata": {
        "id": "595854f8"
      },
      "source": [
        "# HW3 Practical: Comparing Generative Paradigms on CIFAR-10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8329343",
      "metadata": {
        "id": "d8329343"
      },
      "source": [
        "Welcome! This notebook provides the training and evaluation pipeline for the four generative models you will build in `models/`.\n",
        "\n",
        "- Run the setup cells to install dependencies and load CIFAR-10.\n",
        "- Complete the TODOs in the Python modules, verify with Gradescope autograder, then return here to train and evaluate models.\n",
        "- Follow the prompts in each section to log results and save artefacts (sample image grids, metrics, and plots) for inclusion in your PDF report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f41506e2",
      "metadata": {
        "id": "f41506e2"
      },
      "outputs": [],
      "source": [
        "# Install lightweight dependencies (safe to re-run)\n",
        "%pip install --quiet torch-fidelity tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ec1c0eb",
      "metadata": {
        "id": "6ec1c0eb"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from cifar10 import load_cifar10\n",
        "from models.gan import DCGAN\n",
        "from models.vae import ConvVAE\n",
        "from models.pixelcnn import PixelCNN\n",
        "from models.ddpm import DenoiseUNet\n",
        "\n",
        "from torch_fidelity import calculate_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "04846bc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04846bc4",
        "outputId": "11c4d747-47ff-4719-a517-e389bc89e4da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Project root: /content/Asignement3_Autonme2025_cas1\n"
          ]
        }
      ],
      "source": [
        "# Global configuration\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "DATA_ROOT = PROJECT_ROOT\n",
        "ARTIFACT_DIR = PROJECT_ROOT / \"artifacts\"\n",
        "ARTIFACT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_SIZE = 32\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "if DEVICE.type == \"cuda\":\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbca4293",
      "metadata": {
        "id": "fbca4293"
      },
      "source": [
        "### Experiment presets\n",
        "\n",
        "The dictionaries below define the baseline (\"small\") and scaled (\"medium\") configurations used in the assignment. Baselines train for 30 epochs, and the scaled variants run for 40 epochs so you can contrast added capacity with extra compute. Feel free to explore other values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_y6Jo0UFNpJ",
        "outputId": "5afae362-4356-4195-c4dc-1daf0950167a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dcgan': {'small': {'base_channels': 64,\n",
              "   'latent_dim': 128,\n",
              "   'epochs': 30,\n",
              "   'lr': 0.0002},\n",
              "  'medium': {'base_channels': 96,\n",
              "   'latent_dim': 128,\n",
              "   'epochs': 40,\n",
              "   'lr': 0.0002}},\n",
              " 'vae': {'small': {'base_channels': 64,\n",
              "   'latent_dim': 128,\n",
              "   'epochs': 30,\n",
              "   'lr': 0.0002},\n",
              "  'medium': {'base_channels': 96,\n",
              "   'latent_dim': 192,\n",
              "   'epochs': 40,\n",
              "   'lr': 0.0002}},\n",
              " 'pixelcnn': {'small': {'hidden_channels': 64,\n",
              "   'residual_layers': 5,\n",
              "   'epochs': 30,\n",
              "   'lr': 0.0003},\n",
              "  'medium': {'hidden_channels': 96,\n",
              "   'residual_layers': 7,\n",
              "   'epochs': 40,\n",
              "   'lr': 0.0003}},\n",
              " 'ddpm': {'small': {'base_channels': 64,\n",
              "   'time_channels': 256,\n",
              "   'timesteps': 1000,\n",
              "   'epochs': 30,\n",
              "   'lr': 0.0002},\n",
              "  'medium': {'base_channels': 96,\n",
              "   'time_channels': 256,\n",
              "   'timesteps': 750,\n",
              "   'epochs': 40,\n",
              "   'lr': 0.0002}}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "EXPERIMENT_CONFIGS = {\n",
        "    \"dcgan\": {\n",
        "        \"small\": {\"base_channels\": 64, \"latent_dim\": 128, \"epochs\": 30, \"lr\": 2e-4},\n",
        "        \"medium\": {\"base_channels\": 96, \"latent_dim\": 128, \"epochs\": 40, \"lr\": 2e-4},\n",
        "    },\n",
        "    \"vae\": {\n",
        "        \"small\": {\"base_channels\": 64, \"latent_dim\": 128, \"epochs\": 30, \"lr\": 2e-4},\n",
        "        \"medium\": {\"base_channels\": 96, \"latent_dim\": 192, \"epochs\": 40, \"lr\": 2e-4},\n",
        "    },\n",
        "    \"pixelcnn\": {\n",
        "        \"small\": {\"hidden_channels\": 64, \"residual_layers\": 5, \"epochs\": 30, \"lr\": 3e-4},\n",
        "        \"medium\": {\"hidden_channels\": 96, \"residual_layers\": 7, \"epochs\": 40, \"lr\": 3e-4},\n",
        "    },\n",
        "    \"ddpm\": {\n",
        "        \"small\": {\"base_channels\": 64, \"time_channels\": 256, \"timesteps\": 1000, \"epochs\": 30, \"lr\": 2e-4},\n",
        "        \"medium\": {\"base_channels\": 96, \"time_channels\": 256, \"timesteps\": 750, \"epochs\": 40, \"lr\": 2e-4},\n",
        "    },\n",
        "}\n",
        "\n",
        "EXPERIMENT_CONFIGS\n"
      ],
      "id": "9_y6Jo0UFNpJ"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfGoZmG9FNpJ",
        "outputId": "600dd000-5138-4902-cb44-fb2ccbcc48c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dcgan': 1024, 'vae': 1024, 'pixelcnn': 1024, 'ddpm': 1024}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "THROUGHPUT_SAMPLES = {\n",
        "    \"dcgan\": 1024,\n",
        "    \"vae\": 1024,\n",
        "    \"pixelcnn\": 1024,  # autoregressive sampling is slow; document your actual sample count if you change this\n",
        "    \"ddpm\": 1024,\n",
        "}\n",
        "\n",
        "THROUGHPUT_SAMPLES\n"
      ],
      "id": "qfGoZmG9FNpJ"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Asignement3_Autonme2025_cas1\n",
        "\n",
        "# (Re)télécharger l’archive CIFAR-10 au bon endroit\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O cifar-10-python.tar.gz\n",
        "\n",
        "# (Ré)extraire\n",
        "!tar -xzf cifar-10-python.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bxzCapYJttF",
        "outputId": "81ca32cf-3fc4-440b-d703-7c43fadb4e1e"
      },
      "id": "2bxzCapYJttF",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Asignement3_Autonme2025_cas1\n",
            "--2025-12-07 23:30:21--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  61.0MB/s    in 2.7s    \n",
            "\n",
            "2025-12-07 23:30:24 (61.0 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c12062",
      "metadata": {
        "id": "e2c12062"
      },
      "source": [
        "## Data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5msr258NFNpJ",
        "outputId": "9177b3a4-369f-413f-ac1d-7c2c09f1140d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 391, Validation batches: 79\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_dataloaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "    train_loader = load_cifar10(\n",
        "        root=str(DATA_ROOT),\n",
        "        batch_size=batch_size,\n",
        "        train=True,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    val_loader = load_cifar10(\n",
        "        root=str(DATA_ROOT),\n",
        "        batch_size=batch_size,\n",
        "        train=False,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "train_loader, val_loader = get_dataloaders()\n",
        "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n"
      ],
      "id": "5msr258NFNpJ"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDkSU49lFNpJ",
        "outputId": "50352f0b-dd7c-47e4-8752-e24a9be4116f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 391, Validation batches: 79\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_dataloaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "    train_loader = load_cifar10(\n",
        "        root=str(DATA_ROOT),\n",
        "        batch_size=batch_size,\n",
        "        train=True,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    val_loader = load_cifar10(\n",
        "        root=str(DATA_ROOT),\n",
        "        batch_size=batch_size,\n",
        "        train=False,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "set_seed(SEED)\n",
        "train_loader, val_loader = get_dataloaders()\n",
        "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n"
      ],
      "id": "fDkSU49lFNpJ"
    },
    {
      "cell_type": "markdown",
      "id": "c60008d8",
      "metadata": {
        "id": "c60008d8"
      },
      "source": [
        "## Utility functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UW7c3trmFNpK"
      },
      "outputs": [],
      "source": [
        "def to_device(batch, device=DEVICE):\n",
        "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "\n",
        "def images_to_uint8(tensor: torch.Tensor) -> np.ndarray:\n",
        "    tensor = tensor.detach().cpu().clamp(0.0, 1.0)\n",
        "    tensor = (tensor * 255.0).round().to(torch.uint8)\n",
        "    return tensor.permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "\n",
        "def save_image_grid(images: torch.Tensor, path: Path, nrow: int = 8):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    grid = make_grid(images.detach().cpu(), nrow=nrow, padding=2)\n",
        "    save_image(grid, str(path))\n",
        "    return grid\n",
        "\n",
        "\n",
        "def gather_real_images(loader: DataLoader, max_samples: int = 2048) -> torch.Tensor:\n",
        "    batches = []\n",
        "    total = 0\n",
        "    for batch in loader:\n",
        "        imgs = batch[\"images\"]\n",
        "        batches.append(imgs)\n",
        "        total += imgs.size(0)\n",
        "        if total >= max_samples:\n",
        "            break\n",
        "    return torch.cat(batches, dim=0)[:max_samples]\n",
        "\n",
        "\n",
        "def compute_kid_score(real_images: torch.Tensor, fake_images: torch.Tensor) -> float:\n",
        "    real_np = images_to_uint8(real_images)\n",
        "    fake_np = images_to_uint8(fake_images)\n",
        "\n",
        "    from torch.utils.data import Dataset\n",
        "\n",
        "    class _ArrayDataset(Dataset):\n",
        "        def __init__(self, array: np.ndarray):\n",
        "            self.array = array\n",
        "\n",
        "        def __len__(self) -> int:\n",
        "            return self.array.shape[0]\n",
        "\n",
        "        def __getitem__(self, idx: int):\n",
        "            arr = self.array[idx]\n",
        "            tensor = torch.from_numpy(arr).permute(2, 0, 1).contiguous()\n",
        "            return tensor\n",
        "\n",
        "    metrics = calculate_metrics(\n",
        "        input1=_ArrayDataset(fake_np),\n",
        "        input2=_ArrayDataset(real_np),\n",
        "        kid=True,\n",
        "        fid=False,\n",
        "        verbose=False,\n",
        "    )\n",
        "    kid_key = \"kernel_inception_distance_mean\"\n",
        "    if kid_key not in metrics:\n",
        "        kid_key = \"kid_mean\"\n",
        "    kid_value = metrics.get(kid_key)\n",
        "    if kid_value is None:\n",
        "        raise KeyError(f\"KID metric missing expected keys: {list(metrics.keys())}\")\n",
        "    return float(kid_value)\n",
        "\n",
        "\n",
        "def measure_sampling_throughput(sample_fn, num_images: int = 1024, device=DEVICE):\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
        "    start = time.perf_counter()\n",
        "    samples = sample_fn(num_images=num_images, device=device)\n",
        "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
        "    elapsed = time.perf_counter() - start\n",
        "    throughput = num_images / elapsed\n",
        "    return samples, elapsed, throughput\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n"
      ],
      "id": "UW7c3trmFNpK"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "289c6b8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "289c6b8b",
        "outputId": "7f45e7cc-778d-416c-b95d-693384465d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real subset cached: torch.Size([2048, 3, 32, 32]) (device copy: torch.Size([2048, 3, 32, 32]))\n"
          ]
        }
      ],
      "source": [
        "real_subset = gather_real_images(val_loader, max_samples=2048)\n",
        "real_subset_device = real_subset.to(DEVICE)\n",
        "print(f\"Real subset cached: {real_subset.shape} (device copy: {real_subset_device.shape})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c246b7c3",
      "metadata": {
        "id": "c246b7c3"
      },
      "source": [
        "## Training loops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OU_DT_2BFNpK"
      },
      "outputs": [],
      "source": [
        "def _init_history():\n",
        "    return defaultdict(list)\n",
        "\n",
        "def train_dcgan(model: DCGAN, dataloader: DataLoader, optimizer_g, optimizer_d, epochs: int = 5, device=DEVICE):\n",
        "    model.to(device)\n",
        "    history = _init_history()\n",
        "    epoch_times = []\n",
        "    for epoch in range(epochs):\n",
        "        start = time.perf_counter()\n",
        "        progress = tqdm(dataloader, desc=f\"[DCGAN] Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "        for batch in progress:\n",
        "            images = batch[\"images\"].to(device)\n",
        "\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            optimizer_d.zero_grad(set_to_none=True)\n",
        "            out_d = model({\"images\": images})\n",
        "            loss_d = out_d[\"discriminator_loss\"]\n",
        "            loss_d.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            optimizer_g.zero_grad(set_to_none=True)\n",
        "            out_g = model({\"images\": images})\n",
        "            loss_g = out_g[\"generator_loss\"]\n",
        "            loss_g.backward()\n",
        "            optimizer_g.step()\n",
        "\n",
        "            history[\"d_loss\"].append(loss_d.item())\n",
        "            history[\"g_loss\"].append(loss_g.item())\n",
        "            progress.set_postfix({\"d\": loss_d.item(), \"g\": loss_g.item()})\n",
        "        epoch_times.append(time.perf_counter() - start)\n",
        "    history[\"epoch_time\"] = epoch_times\n",
        "    return history\n",
        "\n",
        "\n",
        "def train_vae(model: ConvVAE, dataloader: DataLoader, optimizer, epochs: int = 5, device=DEVICE):\n",
        "    model.to(device)\n",
        "    history = _init_history()\n",
        "    epoch_times = []\n",
        "    for epoch in range(epochs):\n",
        "        start = time.perf_counter()\n",
        "        progress = tqdm(dataloader, desc=f\"[VAE] Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "        for batch in progress:\n",
        "            images = batch[\"images\"].to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            out = model({\"images\": images})\n",
        "            loss = out[\"loss\"]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            history[\"loss\"].append(loss.item())\n",
        "            history[\"kl\"].append(out[\"kl\"].mean().item())\n",
        "            history[\"recon\"].append(out[\"reconstruction_loss\"].mean().item())\n",
        "            progress.set_postfix({\"loss\": loss.item()})\n",
        "        epoch_times.append(time.perf_counter() - start)\n",
        "    history[\"epoch_time\"] = epoch_times\n",
        "    return history\n",
        "\n",
        "\n",
        "def train_pixelcnn(model: PixelCNN, dataloader: DataLoader, optimizer, epochs: int = 5, device=DEVICE):\n",
        "    model.to(device)\n",
        "    history = _init_history()\n",
        "    epoch_times = []\n",
        "    for epoch in range(epochs):\n",
        "        start = time.perf_counter()\n",
        "        progress = tqdm(dataloader, desc=f\"[PixelCNN] Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "        for batch in progress:\n",
        "            images = batch[\"images\"].to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            out = model({\"images\": images})\n",
        "            loss = out[\"loss\"]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            history[\"loss\"].append(loss.item())\n",
        "            progress.set_postfix({\"loss\": loss.item()})\n",
        "        epoch_times.append(time.perf_counter() - start)\n",
        "    history[\"epoch_time\"] = epoch_times\n",
        "    return history\n",
        "\n",
        "\n",
        "def train_ddpm(model: DenoiseUNet, dataloader: DataLoader, optimizer, epochs: int = 1, device=DEVICE):\n",
        "    model.to(device)\n",
        "    history = _init_history()\n",
        "    epoch_times = []\n",
        "    for epoch in range(epochs):\n",
        "        start = time.perf_counter()\n",
        "        progress = tqdm(dataloader, desc=f\"[DDPM] Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "        for batch in progress:\n",
        "            images = batch[\"images\"].to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            out = model({\"images\": images})\n",
        "            loss = out[\"loss\"]\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            history[\"loss\"].append(loss.item())\n",
        "            progress.set_postfix({\"loss\": loss.item()})\n",
        "        epoch_times.append(time.perf_counter() - start)\n",
        "    history[\"epoch_time\"] = epoch_times\n",
        "    return history\n",
        "\n"
      ],
      "id": "OU_DT_2BFNpK"
    },
    {
      "cell_type": "markdown",
      "id": "4ed83abe",
      "metadata": {
        "id": "4ed83abe"
      },
      "source": [
        "## Sampling & evaluation helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5vqCBvMNFNpL"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample_dcgan(model: DCGAN, num_samples: int = 64, device=DEVICE, batch_size: int = 64):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    samples = []\n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        n = min(batch_size, num_samples - start)\n",
        "        z = torch.randn(n, model.latent_dim, device=device)\n",
        "\n",
        "        # fake = ### TODO: sample from the model\n",
        "        fake = model.sample(z)\n",
        "\n",
        "        samples.append(fake.detach().cpu())\n",
        "    return torch.cat(samples, dim=0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_vae(model: ConvVAE, num_samples: int = 64, device=DEVICE, batch_size: int = 64):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    samples = []\n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        n = min(batch_size, num_samples - start)\n",
        "        z = torch.randn(n, model.latent_dim, device=device)\n",
        "\n",
        "        # mean, logvar = ### TODO: decode the latent codes using the model\n",
        "        mean, logvar = model.decode(z)\n",
        "\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "\n",
        "        # draw = ### TODO: sample from the distribution\n",
        "        draw = mean + std * torch.randn_like(std)\n",
        "\n",
        "        samples.append(draw.detach().cpu())\n",
        "    return torch.cat(samples, dim=0).clamp(0.0, 1.0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_pixelcnn(model: PixelCNN, num_samples: int = 16, device=DEVICE, image_size: int = IMAGE_SIZE):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    bins = model.bins\n",
        "    samples = torch.zeros(num_samples, model.image_channels, image_size, image_size, device=device)\n",
        "    for row in range(image_size):\n",
        "        for col in range(image_size):\n",
        "\n",
        "            # logits = ### TODO get the logits from the model\n",
        "            out = model({\"images\": samples})\n",
        "            logits = out[\"logits\"]\n",
        "\n",
        "            logits = logits.view(num_samples, model.image_channels, bins, image_size, image_size)\n",
        "\n",
        "            # probs = ### TODO: Softmax the logits to get the probability of each bin\n",
        "            probs = torch.softmax(logits[:, :, :, row, col], dim=2)\n",
        "\n",
        "            cat = torch.distributions.Categorical(probs=probs)\n",
        "            pixel = cat.sample()\n",
        "            pixel = pixel.float() / (bins - 1)\n",
        "            samples[:, :, row, col] = pixel\n",
        "    return samples.detach().cpu().clamp(0.0, 1.0)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def _ddpm_predict_noise(model: DenoiseUNet, xt: torch.Tensor, t: torch.Tensor):\n",
        "    time_emb = model.time_embedding(t)\n",
        "\n",
        "    # h0 = ### TODO: forward the input through the layers\n",
        "    # skip0, h1 = ### TODO\n",
        "    # skip1, h2 = ### TODO\n",
        "    # skip2, h3 = ### TODO\n",
        "    # h_mid = ### TODO\n",
        "    # h = ### TODO\n",
        "    # h = ### TODO\n",
        "    # h = ### TODO\n",
        "    # pred_noise = ### TODO\n",
        "\n",
        "    h0 = model.model[\"init\"](xt)\n",
        "    skip0, h1 = model.model[\"down0\"](h0, time_emb)\n",
        "    skip1, h2 = model.model[\"down1\"](h1, time_emb)\n",
        "    skip2, h3 = model.model[\"down2\"](h2, time_emb)\n",
        "    h_mid = model.model[\"mid\"](h3, time_emb)\n",
        "    h = model.model[\"up2\"](h_mid, skip2, time_emb)\n",
        "    h = model.model[\"up1\"](h, skip1, time_emb)\n",
        "    h = model.model[\"up0\"](h, skip0, time_emb)\n",
        "    pred_noise = model.model[\"out\"](h)\n",
        "\n",
        "    return pred_noise\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_ddpm(model: DenoiseUNet, num_samples: int = 64, device=DEVICE, timesteps: int = None):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    T = timesteps if timesteps is not None else model.timesteps\n",
        "    betas = model.betas.to(device)\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = model.alphas_cumprod.to(device)\n",
        "    alphas_cumprod_prev = torch.cat([torch.ones(1, device=device), alphas_cumprod[:-1]])\n",
        "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "    sqrt_one_minus_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "    xt = torch.randn(num_samples, model.image_channels, IMAGE_SIZE, IMAGE_SIZE, device=device)\n",
        "    for step in reversed(range(T)):\n",
        "        t = torch.full((num_samples,), step, device=device, dtype=torch.long)\n",
        "\n",
        "        # pred_noise = ### TODO: predict the noise from the model\n",
        "        pred_noise = _ddpm_predict_noise(model, xt, t)\n",
        "\n",
        "        beta_t = betas[step]\n",
        "        sqrt_recip_alpha_t = sqrt_recip_alphas[step]\n",
        "        sqrt_one_minus_cumprod_t = sqrt_one_minus_cumprod[step]\n",
        "        model_mean = sqrt_recip_alpha_t * (xt - beta_t / sqrt_one_minus_cumprod_t * pred_noise)\n",
        "        if step > 0:\n",
        "            variance = beta_t * (1.0 - alphas_cumprod_prev[step]) / (1.0 - alphas_cumprod[step])\n",
        "            noise = torch.randn_like(xt)\n",
        "\n",
        "            # xt = ### TODO: update the image\n",
        "            xt = model_mean + torch.sqrt(variance) * noise\n",
        "\n",
        "        else:\n",
        "            xt = model_mean\n",
        "    return xt.detach().cpu().clamp(-1.0, 1.0)\n",
        "\n"
      ],
      "id": "5vqCBvMNFNpL"
    },
    {
      "cell_type": "markdown",
      "id": "de21c0d6",
      "metadata": {
        "id": "de21c0d6"
      },
      "source": [
        "## Experiment tracking utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fgLDSpfaFNpL"
      },
      "outputs": [],
      "source": [
        "experiment_log = {}\n",
        "\n",
        "\n",
        "def record_metrics(model_name: str, variant: str, metrics: dict):\n",
        "    experiment_log.setdefault(model_name, {})[variant] = metrics\n",
        "\n",
        "\n",
        "def show_metrics(model_name: str = None):\n",
        "    if model_name is None:\n",
        "        for name in experiment_log:\n",
        "            show_metrics(name)\n",
        "        return\n",
        "    print(f\"=== {model_name} ===\")\n",
        "    entries = experiment_log.get(model_name, {})\n",
        "    for variant, metrics in entries.items():\n",
        "        print(f\"  [{variant}]\")\n",
        "        for key, value in metrics.items():\n",
        "            print(f\"    {key}: {value}\")\n",
        "    if not entries:\n",
        "        print(\"  (no entries yet)\")\n"
      ],
      "id": "fgLDSpfaFNpL"
    },
    {
      "cell_type": "markdown",
      "id": "88a9bfb2",
      "metadata": {
        "id": "88a9bfb2"
      },
      "source": [
        "## DCGAN (adversarial generation)\n",
        "\n",
        "Fill in the TODOs inside `models/gan.py` before running this section. The cells below:\n",
        "\n",
        "1. Instantiate the model and optimizers.\n",
        "2. Train for the baseline configuration.\n",
        "3. Generate sample grids, measure KID, and record sampling throughput.\n",
        "4. Repeat with the medium-scale variant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1KupPB3FNpL",
        "outputId": "4cae5b97-72b5-4b92-941f-dd14f61cf866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline DCGAN parameters: 1,846,980\n"
          ]
        }
      ],
      "source": [
        "dcgan_cfg = EXPERIMENT_CONFIGS[\"dcgan\"][\"small\"]\n",
        "dcgan = DCGAN(\n",
        "    image_channels=3,\n",
        "    latent_dim=dcgan_cfg[\"latent_dim\"],\n",
        "    base_channels=dcgan_cfg[\"base_channels\"],\n",
        ")\n",
        "optim_g = torch.optim.Adam(dcgan.generator.parameters(), lr=dcgan_cfg[\"lr\"], betas=(0.5, 0.999))\n",
        "optim_d = torch.optim.Adam(dcgan.discriminator.parameters(), lr=dcgan_cfg[\"lr\"], betas=(0.5, 0.999))\n",
        "print(f\"Baseline DCGAN parameters: {count_parameters(dcgan):,}\")\n"
      ],
      "id": "a1KupPB3FNpL"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440,
          "referenced_widgets": [
            "425bc7f6bf18499eb2adc541f52c9584",
            "c1ec3c99e6344afda412df2d42ddd6af",
            "8e49e12f5d9341188b79b127d93c5766",
            "7852e10358a04763895d710f037acba3",
            "ada745a1220745218f750e75e3405c34",
            "b5b6d53f88114c8fa8ef8dac51ff3f87",
            "cdf07c397c81420481bfffae5f882660",
            "cfa884d0c1f34501b9a5bd165e5d8316",
            "4c14936cf19d41cbb539696af6377d8c",
            "c325e3cf8c154051975754d568a9ffc3",
            "5231c452e901412a81450cac2f35bf5f"
          ]
        },
        "id": "vj1Fyz2gFNpM",
        "outputId": "901710e9-91ac-4fed-f1a2-24fbda84c394"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[DCGAN] Epoch 1/30:   0%|          | 0/391 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "425bc7f6bf18499eb2adc541f52c9584"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4032656727.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDCGAN_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdcgan_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdcgan_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dcgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdcgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_g\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDCGAN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdcgan_epoch_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdcgan_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch_time\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# populate this inside your implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdcgan_train_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdcgan_epoch_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-219516997.py\u001b[0m in \u001b[0;36mtrain_dcgan\u001b[0;34m(model, dataloader, optimizer_g, optimizer_d, epochs, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mout_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mloss_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_g\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generator_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Asignement3_Autonme2025_cas1/models/gan.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# fake_images = ...  # TODO: generate fake images from the latent noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# logits_real = ...  # TODO: score real images with the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Asignement3_Autonme2025_cas1/models/gan.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m   1162\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "DCGAN_EPOCHS = dcgan_cfg[\"epochs\"]\n",
        "\n",
        "dcgan_history = train_dcgan(dcgan, train_loader, optimizer_g=optim_g, optimizer_d=optim_d, epochs=DCGAN_EPOCHS)\n",
        "dcgan_epoch_times = dcgan_history[\"epoch_time\"]  # populate this inside your implementation\n",
        "dcgan_train_time = sum(dcgan_epoch_times)\n",
        "print(f\"Finished DCGAN training: {len(dcgan_history['d_loss'])} steps\")\n",
        "print(f\"DCGAN training time per epoch (s): {[round(t, 2) for t in dcgan_epoch_times]}\")\n"
      ],
      "id": "vj1Fyz2gFNpM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dO5iCT3FNpM"
      },
      "outputs": [],
      "source": [
        "# TODO: run once `sample_dcgan` is implemented.\n",
        "# Evaluation: sampling, KID, throughput\n",
        "samples_64 = sample_dcgan(dcgan, num_samples=64, device=DEVICE)\n",
        "samples_64_vis = (samples_64 + 1.0) / 2.0  # map from [-1, 1] to [0, 1]\n",
        "save_image_grid(samples_64_vis, ARTIFACT_DIR / \"dcgan_samples_baseline.png\", nrow=8)\n",
        "\n",
        "sample_budget = THROUGHPUT_SAMPLES[\"dcgan\"]\n",
        "samples_large, elapsed, throughput = measure_sampling_throughput(\n",
        "    lambda num_images, device: (sample_dcgan(dcgan, num_samples=num_images, device=device) + 1.0) / 2.0,\n",
        "    num_images=sample_budget,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "kid = compute_kid_score(real_subset_device[:samples_large.size(0)].cpu(), samples_large.cpu())\n",
        "record_metrics(\n",
        "    \"DCGAN\",\n",
        "    \"baseline\",\n",
        "    {\n",
        "        \"epochs\": DCGAN_EPOCHS,\n",
        "        \"params\": count_parameters(dcgan),\n",
        "        \"kid\": kid,\n",
        "        \"sampling_time_s\": elapsed,\n",
        "        \"throughput_img_per_s\": throughput,\n",
        "        \"train_time_total_s\": dcgan_train_time,\n",
        "        \"train_time_per_epoch_s\": dcgan_train_time / DCGAN_EPOCHS,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"KID (x10^3): {kid:.3f}\")\n",
        "print(f\"Sampling time for {samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.1f} img/s)\")\n"
      ],
      "id": "-dO5iCT3FNpM"
    },
    {
      "cell_type": "markdown",
      "id": "eb4f782f",
      "metadata": {
        "id": "eb4f782f"
      },
      "source": [
        "> **Scaling experiment:** Retrain the medium-scale configuration in below and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"DCGAN\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93a9204c",
      "metadata": {
        "id": "93a9204c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "43062489",
      "metadata": {
        "id": "43062489"
      },
      "source": [
        "## Convolutional VAE (latent variable model)\n",
        "\n",
        "Make sure the loss components in `models/vae.py` are implemented before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avH6Vn5FFNpM"
      },
      "outputs": [],
      "source": [
        "vae_cfg = EXPERIMENT_CONFIGS[\"vae\"][\"small\"]\n",
        "vae = ConvVAE(\n",
        "    image_channels=3,\n",
        "    latent_dim=vae_cfg[\"latent_dim\"],\n",
        "    base_channels=vae_cfg[\"base_channels\"],\n",
        ")\n",
        "optim_vae = torch.optim.Adam(vae.parameters(), lr=vae_cfg[\"lr\"])\n",
        "print(f\"Baseline VAE parameters: {count_parameters(vae):,}\")\n"
      ],
      "id": "avH6Vn5FFNpM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx246CUpFNpM"
      },
      "outputs": [],
      "source": [
        "VAE_EPOCHS = vae_cfg[\"epochs\"]\n",
        "\n",
        "vae_history = train_vae(vae, train_loader, optimizer=optim_vae, epochs=VAE_EPOCHS)\n",
        "vae_epoch_times = vae_history[\"epoch_time\"]  # populate this inside your implementation\n",
        "vae_train_time = sum(vae_epoch_times)\n",
        "print(f\"Finished VAE training: {len(vae_history['loss'])} steps\")\n",
        "print(f\"VAE training time per epoch (s): {[round(t, 2) for t in vae_epoch_times]}\")\n"
      ],
      "id": "Vx246CUpFNpM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXUyP6ecFNpN"
      },
      "outputs": [],
      "source": [
        "def evaluate_vae_elbo(model: ConvVAE, loader: DataLoader, device=DEVICE):\n",
        "    model.eval()\n",
        "    total_loss = total_recon = total_kl = 0.0\n",
        "    total_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            images = batch[\"images\"].to(device)\n",
        "            out = model({\"images\": images})\n",
        "            batch_size = images.size(0)\n",
        "            total_examples += batch_size\n",
        "            total_loss += out[\"loss\"].item() * batch_size\n",
        "            total_recon += out[\"reconstruction_loss\"].mean().item() * batch_size\n",
        "            total_kl += out[\"kl\"].mean().item() * batch_size\n",
        "    return {\n",
        "        \"loss\": total_loss / total_examples,\n",
        "        \"reconstruction\": total_recon / total_examples,\n",
        "        \"kl\": total_kl / total_examples,\n",
        "    }\n",
        "\n",
        "\n",
        "# TODO: once `sample_vae' are implemented, run the evaluation below.\n",
        "\n",
        "vae_elbo = evaluate_vae_elbo(vae, val_loader)\n",
        "vae_samples = sample_vae(vae, num_samples=64, device=DEVICE)\n",
        "save_image_grid(vae_samples, ARTIFACT_DIR / \"vae_samples_baseline.png\", nrow=8)\n",
        "\n",
        "sample_budget = THROUGHPUT_SAMPLES[\"vae\"]\n",
        "vae_samples_large, elapsed, throughput = measure_sampling_throughput(\n",
        "    lambda num_images, device: sample_vae(vae, num_samples=num_images, device=device),\n",
        "    num_images=sample_budget,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "kid = compute_kid_score(real_subset_device[:vae_samples_large.size(0)].cpu(), vae_samples_large.cpu())\n",
        "record_metrics(\n",
        "    \"VAE\",\n",
        "    \"baseline\",\n",
        "    {\n",
        "        \"epochs\": VAE_EPOCHS,\n",
        "        \"params\": count_parameters(vae),\n",
        "        \"kid\": kid,\n",
        "        \"nll\": vae_elbo[\"loss\"],\n",
        "        \"sampling_time_s\": elapsed,\n",
        "        \"throughput_img_per_s\": throughput,\n",
        "        \"train_time_total_s\": vae_train_time,\n",
        "        \"train_time_per_epoch_s\": vae_train_time / VAE_EPOCHS,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"Validation ELBO: {vae_elbo['loss']:.4f} (recon {vae_elbo['reconstruction']:.4f}, KL {vae_elbo['kl']:.4f})\")\n",
        "print(f\"KID (x10^3): {kid:.3f}\")\n",
        "print(f\"Sampling time for {vae_samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.1f} img/s)\")\n",
        "\n"
      ],
      "id": "TXUyP6ecFNpN"
    },
    {
      "cell_type": "markdown",
      "id": "33f063c7",
      "metadata": {
        "id": "33f063c7"
      },
      "source": [
        "> **Scaling experiment:** Retrain the medium-scale configuration and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"VAE\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df6e80c",
      "metadata": {
        "id": "8df6e80c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7670317f",
      "metadata": {
        "id": "7670317f"
      },
      "source": [
        "## PixelCNN (autoregressive generation)\n",
        "\n",
        "Ensure the masking logic and loss in `models/pixelcnn.py` are implemented.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_Oqyn9SFNpN"
      },
      "outputs": [],
      "source": [
        "pixelcnn_cfg = EXPERIMENT_CONFIGS[\"pixelcnn\"][\"small\"]\n",
        "pixelcnn = PixelCNN(\n",
        "    image_channels=3,\n",
        "    hidden_channels=pixelcnn_cfg[\"hidden_channels\"],\n",
        "    residual_layers=pixelcnn_cfg[\"residual_layers\"],\n",
        "    bins=256,\n",
        ")\n",
        "optim_pixelcnn = torch.optim.Adam(pixelcnn.parameters(), lr=pixelcnn_cfg[\"lr\"])\n",
        "print(f\"Baseline PixelCNN parameters: {count_parameters(pixelcnn):,}\")\n"
      ],
      "id": "Z_Oqyn9SFNpN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKtfS57MFNpN"
      },
      "outputs": [],
      "source": [
        "PIXELCNN_EPOCHS = pixelcnn_cfg[\"epochs\"]\n",
        "\n",
        "pixelcnn_history = train_pixelcnn(pixelcnn, train_loader, optimizer=optim_pixelcnn, epochs=PIXELCNN_EPOCHS)\n",
        "pixelcnn_epoch_times = pixelcnn_history[\"epoch_time\"]  # populate this inside your implementation\n",
        "pixelcnn_train_time = sum(pixelcnn_epoch_times)\n",
        "print(f\"Finished PixelCNN training: {len(pixelcnn_history['loss'])} steps\")\n",
        "print(f\"PixelCNN training time per epoch (s): {[round(t, 2) for t in pixelcnn_epoch_times]}\")\n"
      ],
      "id": "dKtfS57MFNpN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_N9dns4FNpN"
      },
      "outputs": [],
      "source": [
        "def evaluate_pixelcnn_nll(model: PixelCNN, loader: DataLoader, device=DEVICE):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            images = batch[\"images\"].to(device)\n",
        "            out = model({\"images\": images})\n",
        "            loss = out[\"loss\"]\n",
        "            batch_size = images.size(0)\n",
        "            total_examples += batch_size\n",
        "            total_loss += loss.item() * batch_size\n",
        "    return total_loss / total_examples\n",
        "\n",
        "\n",
        "# TODO: run once `sample_pixelcnn` is ready.\n",
        "pixelcnn_nll = evaluate_pixelcnn_nll(pixelcnn, val_loader)\n",
        "pixelcnn_samples = sample_pixelcnn(pixelcnn, num_samples=16, device=DEVICE)\n",
        "save_image_grid(pixelcnn_samples, ARTIFACT_DIR / \"pixelcnn_samples_baseline.png\", nrow=4)\n",
        "\n",
        "sample_budget = THROUGHPUT_SAMPLES[\"pixelcnn\"]\n",
        "pixelcnn_samples_large, elapsed, throughput = measure_sampling_throughput(\n",
        "    lambda num_images, device: sample_pixelcnn(pixelcnn, num_samples=num_images, device=device),\n",
        "    num_images=sample_budget,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "kid = compute_kid_score(real_subset_device[:pixelcnn_samples_large.size(0)].cpu(), pixelcnn_samples_large.cpu())\n",
        "record_metrics(\n",
        "    \"PixelCNN\",\n",
        "    \"baseline\",\n",
        "    {\n",
        "        \"epochs\": PIXELCNN_EPOCHS,\n",
        "        \"params\": count_parameters(pixelcnn),\n",
        "        \"kid\": kid,\n",
        "        \"nll\": pixelcnn_nll,\n",
        "        \"sampling_time_s\": elapsed,\n",
        "        \"throughput_img_per_s\": throughput,\n",
        "        \"train_time_total_s\": pixelcnn_train_time,\n",
        "        \"train_time_per_epoch_s\": pixelcnn_train_time / PIXELCNN_EPOCHS,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"Validation NLL: {pixelcnn_nll:.4f}\")\n",
        "print(f\"KID (x10^3): {kid:.3f}\")\n",
        "print(f\"Sampling time for {pixelcnn_samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.2f} img/s)\")\n"
      ],
      "id": "y_N9dns4FNpN"
    },
    {
      "cell_type": "markdown",
      "id": "16e21e0a",
      "metadata": {
        "id": "16e21e0a"
      },
      "source": [
        "> **Scaling experiment:** > **Scaling experiment:** Retrain the medium-scale configuration in below and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"PixelCNN\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f99a441",
      "metadata": {
        "id": "2f99a441"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "733efc6d",
      "metadata": {
        "id": "733efc6d"
      },
      "source": [
        "## DDPM (diffusion model)\n",
        "\n",
        "Verify the diffusion loss path in `models/ddpm.py` before running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OGM-6ZNFNpU"
      },
      "outputs": [],
      "source": [
        "ddpm_cfg = EXPERIMENT_CONFIGS[\"ddpm\"][\"small\"]\n",
        "ddpm = DenoiseUNet(\n",
        "    image_channels=3,\n",
        "    base_channels=ddpm_cfg[\"base_channels\"],\n",
        "    time_channels=ddpm_cfg[\"time_channels\"],\n",
        "    timesteps=ddpm_cfg[\"timesteps\"],\n",
        ")\n",
        "optim_ddpm = torch.optim.Adam(ddpm.parameters(), lr=ddpm_cfg[\"lr\"])\n",
        "print(f\"Baseline DDPM parameters: {count_parameters(ddpm):,}\")\n"
      ],
      "id": "3OGM-6ZNFNpU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njqa6Rb7FNpV"
      },
      "outputs": [],
      "source": [
        "DDPM_EPOCHS = ddpm_cfg[\"epochs\"]\n",
        "\n",
        "ddpm_history = train_ddpm(ddpm, train_loader, optimizer=optim_ddpm, epochs=DDPM_EPOCHS)\n",
        "ddpm_epoch_times = ddpm_history[\"epoch_time\"]  # populate this inside your implementation\n",
        "ddpm_train_time = sum(ddpm_epoch_times)\n",
        "print(f\"Finished DDPM training: {len(ddpm_history['loss'])} steps\")\n",
        "print(f\"DDPM training time per epoch (s): {[round(t, 2) for t in ddpm_epoch_times]}\")\n"
      ],
      "id": "Njqa6Rb7FNpV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KfSMKSZFNpV"
      },
      "outputs": [],
      "source": [
        "# TODO: once `sample_ddpm` is implemented, run the evaluation block below.\n",
        "ddpm_samples = sample_ddpm(ddpm, num_samples=64, device=DEVICE)\n",
        "ddpm_samples_vis = (ddpm_samples + 1.0) / 2.0\n",
        "save_image_grid(ddpm_samples_vis, ARTIFACT_DIR / \"ddpm_samples_baseline.png\", nrow=8)\n",
        "\n",
        "sample_budget = THROUGHPUT_SAMPLES[\"ddpm\"]\n",
        "samples_large, elapsed, throughput = measure_sampling_throughput(\n",
        "    lambda num_images, device: (sample_ddpm(ddpm, num_samples=num_images, device=device) + 1.0) / 2.0,\n",
        "    num_images=sample_budget,\n",
        "    device=DEVICE,\n",
        ")\n",
        "\n",
        "kid = compute_kid_score(real_subset_device[:samples_large.size(0)].cpu(), samples_large.cpu())\n",
        "record_metrics(\n",
        "    \"DDPM\",\n",
        "    \"baseline\",\n",
        "    {\n",
        "        \"epochs\": DDPM_EPOCHS,\n",
        "        \"params\": count_parameters(ddpm),\n",
        "        \"kid\": kid,\n",
        "        \"sampling_time_s\": elapsed,\n",
        "        \"throughput_img_per_s\": throughput,\n",
        "        \"train_time_total_s\": ddpm_train_time,\n",
        "        \"train_time_per_epoch_s\": ddpm_train_time / DDPM_EPOCHS,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"KID (x10^3): {kid:.3f}\")\n",
        "print(f\"Sampling time for {samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.2f} img/s)\")\n"
      ],
      "id": "4KfSMKSZFNpV"
    },
    {
      "cell_type": "markdown",
      "id": "f2f7a276",
      "metadata": {
        "id": "f2f7a276"
      },
      "source": [
        "> **Scaling experiment:** Retrain the medium-scale configuration in below and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"DDPM\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfba670",
      "metadata": {
        "id": "5dfba670"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a274362b",
      "metadata": {
        "id": "a274362b"
      },
      "source": [
        "## Summary & export\n",
        "\n",
        "After running the experiments above (small and medium variants), use the helper below to view logged metrics and export them to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc9YNyGnFNpW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "show_metrics()\n",
        "\n",
        "rows = []\n",
        "for model_name, variants in experiment_log.items():\n",
        "    for variant, metrics in variants.items():\n",
        "        row = {\"model\": model_name, \"variant\": variant}\n",
        "        row.update(metrics)\n",
        "        rows.append(row)\n",
        "if rows:\n",
        "    df_metrics = pd.DataFrame(rows)\n",
        "    display(df_metrics.set_index([\"model\", \"variant\"]))\n",
        "\n",
        "summary_path = ARTIFACT_DIR / \"metrics_summary.json\"\n",
        "with summary_path.open(\"w\") as fp:\n",
        "    json.dump(experiment_log, fp, indent=2)\n",
        "print(f\"Saved metrics summary to {summary_path}\")\n"
      ],
      "id": "Pc9YNyGnFNpW"
    },
    {
      "cell_type": "markdown",
      "id": "c3c40f0f",
      "metadata": {
        "id": "c3c40f0f"
      },
      "source": [
        "## Aggregate plots\n",
        "\n",
        "Use this section to generate the plots and figures requested in the assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b3787a5",
      "metadata": {
        "id": "8b3787a5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "425bc7f6bf18499eb2adc541f52c9584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1ec3c99e6344afda412df2d42ddd6af",
              "IPY_MODEL_8e49e12f5d9341188b79b127d93c5766",
              "IPY_MODEL_7852e10358a04763895d710f037acba3"
            ],
            "layout": "IPY_MODEL_ada745a1220745218f750e75e3405c34"
          }
        },
        "c1ec3c99e6344afda412df2d42ddd6af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b6d53f88114c8fa8ef8dac51ff3f87",
            "placeholder": "​",
            "style": "IPY_MODEL_cdf07c397c81420481bfffae5f882660",
            "value": "[DCGAN] Epoch 1/30:   5%"
          }
        },
        "8e49e12f5d9341188b79b127d93c5766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa884d0c1f34501b9a5bd165e5d8316",
            "max": 391,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c14936cf19d41cbb539696af6377d8c",
            "value": 19
          }
        },
        "7852e10358a04763895d710f037acba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c325e3cf8c154051975754d568a9ffc3",
            "placeholder": "​",
            "style": "IPY_MODEL_5231c452e901412a81450cac2f35bf5f",
            "value": " 19/391 [00:42&lt;12:15,  1.98s/it, d=0.061, g=4.63]"
          }
        },
        "ada745a1220745218f750e75e3405c34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5b6d53f88114c8fa8ef8dac51ff3f87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdf07c397c81420481bfffae5f882660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfa884d0c1f34501b9a5bd165e5d8316": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c14936cf19d41cbb539696af6377d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c325e3cf8c154051975754d568a9ffc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5231c452e901412a81450cac2f35bf5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}