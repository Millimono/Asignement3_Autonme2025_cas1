{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595854f8",
   "metadata": {},
   "source": [
    "# HW3 Practical: Comparing Generative Paradigms on CIFAR-10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8329343",
   "metadata": {},
   "source": [
    "Welcome! This notebook provides the training and evaluation pipeline for the four generative models you will build in `models/`.\n",
    "\n",
    "- Run the setup cells to install dependencies and load CIFAR-10.\n",
    "- Complete the TODOs in the Python modules, verify with Gradescope autograder, then return here to train and evaluate models.\n",
    "- Follow the prompts in each section to log results and save artefacts (sample image grids, metrics, and plots) for inclusion in your PDF report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41506e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install lightweight dependencies (safe to re-run)\n",
    "%pip install --quiet torch-fidelity tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec1c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from cifar10 import load_cifar10\n",
    "from models.gan import DCGAN\n",
    "from models.vae import ConvVAE\n",
    "from models.pixelcnn import PixelCNN\n",
    "from models.ddpm import DenoiseUNet\n",
    "\n",
    "from torch_fidelity import calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04846bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_ROOT = PROJECT_ROOT\n",
    "ARTIFACT_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca4293",
   "metadata": {},
   "source": [
    "### Experiment presets\n",
    "\n",
    "The dictionaries below define the baseline (\"small\") and scaled (\"medium\") configurations used in the assignment. Baselines train for 30 epochs, and the scaled variants run for 40 epochs so you can contrast added capacity with extra compute. Feel free to explore other values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_CONFIGS = {\n",
    "    \"dcgan\": {\n",
    "        \"small\": {\"base_channels\": 64, \"latent_dim\": 128, \"epochs\": 30, \"lr\": 2e-4},\n",
    "        \"medium\": {\"base_channels\": 96, \"latent_dim\": 128, \"epochs\": 40, \"lr\": 2e-4},\n",
    "    },\n",
    "    \"vae\": {\n",
    "        \"small\": {\"base_channels\": 64, \"latent_dim\": 128, \"epochs\": 30, \"lr\": 2e-4},\n",
    "        \"medium\": {\"base_channels\": 96, \"latent_dim\": 192, \"epochs\": 40, \"lr\": 2e-4},\n",
    "    },\n",
    "    \"pixelcnn\": {\n",
    "        \"small\": {\"hidden_channels\": 64, \"residual_layers\": 5, \"epochs\": 30, \"lr\": 3e-4},\n",
    "        \"medium\": {\"hidden_channels\": 96, \"residual_layers\": 7, \"epochs\": 40, \"lr\": 3e-4},\n",
    "    },\n",
    "    \"ddpm\": {\n",
    "        \"small\": {\"base_channels\": 64, \"time_channels\": 256, \"timesteps\": 1000, \"epochs\": 30, \"lr\": 2e-4},\n",
    "        \"medium\": {\"base_channels\": 96, \"time_channels\": 256, \"timesteps\": 750, \"epochs\": 40, \"lr\": 2e-4},\n",
    "    },\n",
    "}\n",
    "\n",
    "EXPERIMENT_CONFIGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THROUGHPUT_SAMPLES = {\n",
    "    \"dcgan\": 1024,\n",
    "    \"vae\": 1024,\n",
    "    \"pixelcnn\": 1024,  # autoregressive sampling is slow; document your actual sample count if you change this\n",
    "    \"ddpm\": 1024,\n",
    "}\n",
    "\n",
    "THROUGHPUT_SAMPLES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c12062",
   "metadata": {},
   "source": [
    "## Data loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    train_loader = load_cifar10(\n",
    "        root=str(DATA_ROOT),\n",
    "        batch_size=batch_size,\n",
    "        train=True,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = load_cifar10(\n",
    "        root=str(DATA_ROOT),\n",
    "        batch_size=batch_size,\n",
    "        train=False,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "train_loader, val_loader = get_dataloaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def get_dataloaders(batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "    train_loader = load_cifar10(\n",
    "        root=str(DATA_ROOT),\n",
    "        batch_size=batch_size,\n",
    "        train=True,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = load_cifar10(\n",
    "        root=str(DATA_ROOT),\n",
    "        batch_size=batch_size,\n",
    "        train=False,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "train_loader, val_loader = get_dataloaders()\n",
    "print(f\"Train batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60008d8",
   "metadata": {},
   "source": [
    "## Utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(batch, device=DEVICE):\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def images_to_uint8(tensor: torch.Tensor) -> np.ndarray:\n",
    "    tensor = tensor.detach().cpu().clamp(0.0, 1.0)\n",
    "    tensor = (tensor * 255.0).round().to(torch.uint8)\n",
    "    return tensor.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "\n",
    "def save_image_grid(images: torch.Tensor, path: Path, nrow: int = 8):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    grid = make_grid(images.detach().cpu(), nrow=nrow, padding=2)\n",
    "    save_image(grid, str(path))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def gather_real_images(loader: DataLoader, max_samples: int = 2048) -> torch.Tensor:\n",
    "    batches = []\n",
    "    total = 0\n",
    "    for batch in loader:\n",
    "        imgs = batch[\"images\"]\n",
    "        batches.append(imgs)\n",
    "        total += imgs.size(0)\n",
    "        if total >= max_samples:\n",
    "            break\n",
    "    return torch.cat(batches, dim=0)[:max_samples]\n",
    "\n",
    "\n",
    "def compute_kid_score(real_images: torch.Tensor, fake_images: torch.Tensor) -> float:\n",
    "    real_np = images_to_uint8(real_images)\n",
    "    fake_np = images_to_uint8(fake_images)\n",
    "\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class _ArrayDataset(Dataset):\n",
    "        def __init__(self, array: np.ndarray):\n",
    "            self.array = array\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            return self.array.shape[0]\n",
    "\n",
    "        def __getitem__(self, idx: int):\n",
    "            arr = self.array[idx]\n",
    "            tensor = torch.from_numpy(arr).permute(2, 0, 1).contiguous()\n",
    "            return tensor\n",
    "\n",
    "    metrics = calculate_metrics(\n",
    "        input1=_ArrayDataset(fake_np),\n",
    "        input2=_ArrayDataset(real_np),\n",
    "        kid=True,\n",
    "        fid=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    kid_key = \"kernel_inception_distance_mean\"\n",
    "    if kid_key not in metrics:\n",
    "        kid_key = \"kid_mean\"\n",
    "    kid_value = metrics.get(kid_key)\n",
    "    if kid_value is None:\n",
    "        raise KeyError(f\"KID metric missing expected keys: {list(metrics.keys())}\")\n",
    "    return float(kid_value)\n",
    "\n",
    "\n",
    "def measure_sampling_throughput(sample_fn, num_images: int = 1024, device=DEVICE):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    start = time.perf_counter()\n",
    "    samples = sample_fn(num_images=num_images, device=device)\n",
    "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
    "    elapsed = time.perf_counter() - start\n",
    "    throughput = num_images / elapsed\n",
    "    return samples, elapsed, throughput\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_subset = gather_real_images(val_loader, max_samples=2048)\n",
    "real_subset_device = real_subset.to(DEVICE)\n",
    "print(f\"Real subset cached: {real_subset.shape} (device copy: {real_subset_device.shape})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246b7c3",
   "metadata": {},
   "source": [
    "## Training loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_history():\n",
    "    return defaultdict(list)\n",
    "  \n",
    "def train_dcgan(model: DCGAN, dataloader: DataLoader, optimizer_g, optimizer_d, epochs: int = 5, device=DEVICE):\n",
    "    model.to(device)\n",
    "    history = _init_history()\n",
    "    epoch_times = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.perf_counter()\n",
    "        progress = tqdm(dataloader, desc=f\"[DCGAN] Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images = batch[\"images\"].to(device)\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            optimizer_d.zero_grad(set_to_none=True)\n",
    "            out_d = model({\"images\": images})\n",
    "            loss_d = out_d[\"discriminator_loss\"]\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            optimizer_g.zero_grad(set_to_none=True)\n",
    "            out_g = model({\"images\": images})\n",
    "            loss_g = out_g[\"generator_loss\"]\n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            history[\"d_loss\"].append(loss_d.item())\n",
    "            history[\"g_loss\"].append(loss_g.item())\n",
    "            progress.set_postfix({\"d\": loss_d.item(), \"g\": loss_g.item()})\n",
    "        epoch_times.append(time.perf_counter() - start)\n",
    "    history[\"epoch_time\"] = epoch_times\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_vae(model: ConvVAE, dataloader: DataLoader, optimizer, epochs: int = 5, device=DEVICE):\n",
    "    model.to(device)\n",
    "    history = _init_history()\n",
    "    epoch_times = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.perf_counter()\n",
    "        progress = tqdm(dataloader, desc=f\"[VAE] Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            out = model({\"images\": images})\n",
    "            loss = out[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            history[\"loss\"].append(loss.item())\n",
    "            history[\"kl\"].append(out[\"kl\"].mean().item())\n",
    "            history[\"recon\"].append(out[\"reconstruction_loss\"].mean().item())\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "        epoch_times.append(time.perf_counter() - start)\n",
    "    history[\"epoch_time\"] = epoch_times\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_pixelcnn(model: PixelCNN, dataloader: DataLoader, optimizer, epochs: int = 5, device=DEVICE):\n",
    "    model.to(device)\n",
    "    history = _init_history()\n",
    "    epoch_times = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.perf_counter()\n",
    "        progress = tqdm(dataloader, desc=f\"[PixelCNN] Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            out = model({\"images\": images})\n",
    "            loss = out[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            history[\"loss\"].append(loss.item())\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "        epoch_times.append(time.perf_counter() - start)\n",
    "    history[\"epoch_time\"] = epoch_times\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_ddpm(model: DenoiseUNet, dataloader: DataLoader, optimizer, epochs: int = 1, device=DEVICE):\n",
    "    model.to(device)\n",
    "    history = _init_history()\n",
    "    epoch_times = []\n",
    "    for epoch in range(epochs):\n",
    "        start = time.perf_counter()\n",
    "        progress = tqdm(dataloader, desc=f\"[DDPM] Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for batch in progress:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            out = model({\"images\": images})\n",
    "            loss = out[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            history[\"loss\"].append(loss.item())\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "        epoch_times.append(time.perf_counter() - start)\n",
    "    history[\"epoch_time\"] = epoch_times\n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed83abe",
   "metadata": {},
   "source": [
    "## Sampling & evaluation helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_dcgan(model: DCGAN, num_samples: int = 64, device=DEVICE, batch_size: int = 64):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    samples = []\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        n = min(batch_size, num_samples - start)\n",
    "        z = torch.randn(n, model.latent_dim, device=device)\n",
    "        fake = ### TODO: sample from the model\n",
    "        samples.append(fake.detach().cpu())\n",
    "    return torch.cat(samples, dim=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_vae(model: ConvVAE, num_samples: int = 64, device=DEVICE, batch_size: int = 64):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    samples = []\n",
    "    for start in range(0, num_samples, batch_size):\n",
    "        n = min(batch_size, num_samples - start)\n",
    "        z = torch.randn(n, model.latent_dim, device=device)\n",
    "        mean, logvar = ### TODO: decode the latent codes using the model\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        draw = ### TODO: sample from the distribution\n",
    "        samples.append(draw.detach().cpu())\n",
    "    return torch.cat(samples, dim=0).clamp(0.0, 1.0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_pixelcnn(model: PixelCNN, num_samples: int = 16, device=DEVICE, image_size: int = IMAGE_SIZE):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    bins = model.bins\n",
    "    samples = torch.zeros(num_samples, model.image_channels, image_size, image_size, device=device)\n",
    "    for row in range(image_size):\n",
    "        for col in range(image_size):\n",
    "            logits = ### TODO get the logits from the model\n",
    "            logits = logits.view(num_samples, model.image_channels, bins, image_size, image_size)\n",
    "            probs = ### TODO: Softmax the logits to get the probability of each bin\n",
    "            cat = torch.distributions.Categorical(probs=probs)\n",
    "            pixel = cat.sample()\n",
    "            pixel = pixel.float() / (bins - 1)\n",
    "            samples[:, :, row, col] = pixel\n",
    "    return samples.detach().cpu().clamp(0.0, 1.0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _ddpm_predict_noise(model: DenoiseUNet, xt: torch.Tensor, t: torch.Tensor):\n",
    "    time_emb = model.time_embedding(t)\n",
    "    h0 = ### TODO: forward the input through the layers\n",
    "    skip0, h1 = ### TODO\n",
    "    skip1, h2 = ### TODO\n",
    "    skip2, h3 = ### TODO\n",
    "    h_mid = ### TODO\n",
    "    h = ### TODO\n",
    "    h = ### TODO\n",
    "    h = ### TODO\n",
    "    pred_noise = ### TODO\n",
    "    return pred_noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(model: DenoiseUNet, num_samples: int = 64, device=DEVICE, timesteps: int = None):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    T = timesteps if timesteps is not None else model.timesteps\n",
    "    betas = model.betas.to(device)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = model.alphas_cumprod.to(device)\n",
    "    alphas_cumprod_prev = torch.cat([torch.ones(1, device=device), alphas_cumprod[:-1]])\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "    sqrt_one_minus_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "\n",
    "    xt = torch.randn(num_samples, model.image_channels, IMAGE_SIZE, IMAGE_SIZE, device=device)\n",
    "    for step in reversed(range(T)):\n",
    "        t = torch.full((num_samples,), step, device=device, dtype=torch.long)\n",
    "        pred_noise = ### TODO: predict the noise from the model\n",
    "        beta_t = betas[step]\n",
    "        sqrt_recip_alpha_t = sqrt_recip_alphas[step]\n",
    "        sqrt_one_minus_cumprod_t = sqrt_one_minus_cumprod[step]\n",
    "        model_mean = sqrt_recip_alpha_t * (xt - beta_t / sqrt_one_minus_cumprod_t * pred_noise)\n",
    "        if step > 0:\n",
    "            variance = beta_t * (1.0 - alphas_cumprod_prev[step]) / (1.0 - alphas_cumprod[step])\n",
    "            noise = torch.randn_like(xt)\n",
    "            xt = ### TODO: update the image\n",
    "        else:\n",
    "            xt = model_mean\n",
    "    return xt.detach().cpu().clamp(-1.0, 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21c0d6",
   "metadata": {},
   "source": [
    "## Experiment tracking utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_log = {}\n",
    "\n",
    "\n",
    "def record_metrics(model_name: str, variant: str, metrics: dict):\n",
    "    experiment_log.setdefault(model_name, {})[variant] = metrics\n",
    "\n",
    "\n",
    "def show_metrics(model_name: str = None):\n",
    "    if model_name is None:\n",
    "        for name in experiment_log:\n",
    "            show_metrics(name)\n",
    "        return\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    entries = experiment_log.get(model_name, {})\n",
    "    for variant, metrics in entries.items():\n",
    "        print(f\"  [{variant}]\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "    if not entries:\n",
    "        print(\"  (no entries yet)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9bfb2",
   "metadata": {},
   "source": [
    "## DCGAN (adversarial generation)\n",
    "\n",
    "Fill in the TODOs inside `models/gan.py` before running this section. The cells below:\n",
    "\n",
    "1. Instantiate the model and optimizers.\n",
    "2. Train for the baseline configuration.\n",
    "3. Generate sample grids, measure KID, and record sampling throughput.\n",
    "4. Repeat with the medium-scale variant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcgan_cfg = EXPERIMENT_CONFIGS[\"dcgan\"][\"small\"]\n",
    "dcgan = DCGAN(\n",
    "    image_channels=3,\n",
    "    latent_dim=dcgan_cfg[\"latent_dim\"],\n",
    "    base_channels=dcgan_cfg[\"base_channels\"],\n",
    ")\n",
    "optim_g = torch.optim.Adam(dcgan.generator.parameters(), lr=dcgan_cfg[\"lr\"], betas=(0.5, 0.999))\n",
    "optim_d = torch.optim.Adam(dcgan.discriminator.parameters(), lr=dcgan_cfg[\"lr\"], betas=(0.5, 0.999))\n",
    "print(f\"Baseline DCGAN parameters: {count_parameters(dcgan):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DCGAN_EPOCHS = dcgan_cfg[\"epochs\"]\n",
    "\n",
    "dcgan_history = train_dcgan(dcgan, train_loader, optimizer_g=optim_g, optimizer_d=optim_d, epochs=DCGAN_EPOCHS)\n",
    "dcgan_epoch_times = dcgan_history[\"epoch_time\"]  # populate this inside your implementation\n",
    "dcgan_train_time = sum(dcgan_epoch_times)\n",
    "print(f\"Finished DCGAN training: {len(dcgan_history['d_loss'])} steps\")\n",
    "print(f\"DCGAN training time per epoch (s): {[round(t, 2) for t in dcgan_epoch_times]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run once `sample_dcgan` is implemented.\n",
    "# Evaluation: sampling, KID, throughput\n",
    "samples_64 = sample_dcgan(dcgan, num_samples=64, device=DEVICE)\n",
    "samples_64_vis = (samples_64 + 1.0) / 2.0  # map from [-1, 1] to [0, 1]\n",
    "save_image_grid(samples_64_vis, ARTIFACT_DIR / \"dcgan_samples_baseline.png\", nrow=8)\n",
    "\n",
    "sample_budget = THROUGHPUT_SAMPLES[\"dcgan\"]\n",
    "samples_large, elapsed, throughput = measure_sampling_throughput(\n",
    "    lambda num_images, device: (sample_dcgan(dcgan, num_samples=num_images, device=device) + 1.0) / 2.0,\n",
    "    num_images=sample_budget,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "kid = compute_kid_score(real_subset_device[:samples_large.size(0)].cpu(), samples_large.cpu())\n",
    "record_metrics(\n",
    "    \"DCGAN\",\n",
    "    \"baseline\",\n",
    "    {\n",
    "        \"epochs\": DCGAN_EPOCHS,\n",
    "        \"params\": count_parameters(dcgan),\n",
    "        \"kid\": kid,\n",
    "        \"sampling_time_s\": elapsed,\n",
    "        \"throughput_img_per_s\": throughput,\n",
    "        \"train_time_total_s\": dcgan_train_time,\n",
    "        \"train_time_per_epoch_s\": dcgan_train_time / DCGAN_EPOCHS,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"KID (x10^3): {kid:.3f}\")\n",
    "print(f\"Sampling time for {samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.1f} img/s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f782f",
   "metadata": {},
   "source": [
    "> **Scaling experiment:** Retrain the medium-scale configuration in below and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"DCGAN\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a9204c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43062489",
   "metadata": {},
   "source": [
    "## Convolutional VAE (latent variable model)\n",
    "\n",
    "Make sure the loss components in `models/vae.py` are implemented before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_cfg = EXPERIMENT_CONFIGS[\"vae\"][\"small\"]\n",
    "vae = ConvVAE(\n",
    "    image_channels=3,\n",
    "    latent_dim=vae_cfg[\"latent_dim\"],\n",
    "    base_channels=vae_cfg[\"base_channels\"],\n",
    ")\n",
    "optim_vae = torch.optim.Adam(vae.parameters(), lr=vae_cfg[\"lr\"])\n",
    "print(f\"Baseline VAE parameters: {count_parameters(vae):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_EPOCHS = vae_cfg[\"epochs\"]\n",
    "\n",
    "vae_history = train_vae(vae, train_loader, optimizer=optim_vae, epochs=VAE_EPOCHS)\n",
    "vae_epoch_times = vae_history[\"epoch_time\"]  # populate this inside your implementation\n",
    "vae_train_time = sum(vae_epoch_times)\n",
    "print(f\"Finished VAE training: {len(vae_history['loss'])} steps\")\n",
    "print(f\"VAE training time per epoch (s): {[round(t, 2) for t in vae_epoch_times]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_vae_elbo(model: ConvVAE, loader: DataLoader, device=DEVICE):\n",
    "    model.eval()\n",
    "    total_loss = total_recon = total_kl = 0.0\n",
    "    total_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            out = model({\"images\": images})\n",
    "            batch_size = images.size(0)\n",
    "            total_examples += batch_size\n",
    "            total_loss += out[\"loss\"].item() * batch_size\n",
    "            total_recon += out[\"reconstruction_loss\"].mean().item() * batch_size\n",
    "            total_kl += out[\"kl\"].mean().item() * batch_size\n",
    "    return {\n",
    "        \"loss\": total_loss / total_examples,\n",
    "        \"reconstruction\": total_recon / total_examples,\n",
    "        \"kl\": total_kl / total_examples,\n",
    "    }\n",
    "\n",
    "\n",
    "# TODO: once `sample_vae' are implemented, run the evaluation below.\n",
    "\n",
    "vae_elbo = evaluate_vae_elbo(vae, val_loader)\n",
    "vae_samples = sample_vae(vae, num_samples=64, device=DEVICE)\n",
    "save_image_grid(vae_samples, ARTIFACT_DIR / \"vae_samples_baseline.png\", nrow=8)\n",
    "\n",
    "sample_budget = THROUGHPUT_SAMPLES[\"vae\"]\n",
    "vae_samples_large, elapsed, throughput = measure_sampling_throughput(\n",
    "    lambda num_images, device: sample_vae(vae, num_samples=num_images, device=device),\n",
    "    num_images=sample_budget,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "kid = compute_kid_score(real_subset_device[:vae_samples_large.size(0)].cpu(), vae_samples_large.cpu())\n",
    "record_metrics(\n",
    "    \"VAE\",\n",
    "    \"baseline\",\n",
    "    {\n",
    "        \"epochs\": VAE_EPOCHS,\n",
    "        \"params\": count_parameters(vae),\n",
    "        \"kid\": kid,\n",
    "        \"nll\": vae_elbo[\"loss\"],\n",
    "        \"sampling_time_s\": elapsed,\n",
    "        \"throughput_img_per_s\": throughput,\n",
    "        \"train_time_total_s\": vae_train_time,\n",
    "        \"train_time_per_epoch_s\": vae_train_time / VAE_EPOCHS,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Validation ELBO: {vae_elbo['loss']:.4f} (recon {vae_elbo['reconstruction']:.4f}, KL {vae_elbo['kl']:.4f})\")\n",
    "print(f\"KID (x10^3): {kid:.3f}\")\n",
    "print(f\"Sampling time for {vae_samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.1f} img/s)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f063c7",
   "metadata": {},
   "source": [
    "> **Scaling experiment:** Retrain the medium-scale configuration and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"VAE\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6e80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7670317f",
   "metadata": {},
   "source": [
    "## PixelCNN (autoregressive generation)\n",
    "\n",
    "Ensure the masking logic and loss in `models/pixelcnn.py` are implemented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelcnn_cfg = EXPERIMENT_CONFIGS[\"pixelcnn\"][\"small\"]\n",
    "pixelcnn = PixelCNN(\n",
    "    image_channels=3,\n",
    "    hidden_channels=pixelcnn_cfg[\"hidden_channels\"],\n",
    "    residual_layers=pixelcnn_cfg[\"residual_layers\"],\n",
    "    bins=256,\n",
    ")\n",
    "optim_pixelcnn = torch.optim.Adam(pixelcnn.parameters(), lr=pixelcnn_cfg[\"lr\"])\n",
    "print(f\"Baseline PixelCNN parameters: {count_parameters(pixelcnn):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIXELCNN_EPOCHS = pixelcnn_cfg[\"epochs\"]\n",
    "\n",
    "pixelcnn_history = train_pixelcnn(pixelcnn, train_loader, optimizer=optim_pixelcnn, epochs=PIXELCNN_EPOCHS)\n",
    "pixelcnn_epoch_times = pixelcnn_history[\"epoch_time\"]  # populate this inside your implementation\n",
    "pixelcnn_train_time = sum(pixelcnn_epoch_times)\n",
    "print(f\"Finished PixelCNN training: {len(pixelcnn_history['loss'])} steps\")\n",
    "print(f\"PixelCNN training time per epoch (s): {[round(t, 2) for t in pixelcnn_epoch_times]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pixelcnn_nll(model: PixelCNN, loader: DataLoader, device=DEVICE):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch[\"images\"].to(device)\n",
    "            out = model({\"images\": images})\n",
    "            loss = out[\"loss\"]\n",
    "            batch_size = images.size(0)\n",
    "            total_examples += batch_size\n",
    "            total_loss += loss.item() * batch_size\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "# TODO: run once `sample_pixelcnn` is ready.\n",
    "pixelcnn_nll = evaluate_pixelcnn_nll(pixelcnn, val_loader)\n",
    "pixelcnn_samples = sample_pixelcnn(pixelcnn, num_samples=16, device=DEVICE)\n",
    "save_image_grid(pixelcnn_samples, ARTIFACT_DIR / \"pixelcnn_samples_baseline.png\", nrow=4)\n",
    "\n",
    "sample_budget = THROUGHPUT_SAMPLES[\"pixelcnn\"]\n",
    "pixelcnn_samples_large, elapsed, throughput = measure_sampling_throughput(\n",
    "    lambda num_images, device: sample_pixelcnn(pixelcnn, num_samples=num_images, device=device),\n",
    "    num_images=sample_budget,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "kid = compute_kid_score(real_subset_device[:pixelcnn_samples_large.size(0)].cpu(), pixelcnn_samples_large.cpu())\n",
    "record_metrics(\n",
    "    \"PixelCNN\",\n",
    "    \"baseline\",\n",
    "    {\n",
    "        \"epochs\": PIXELCNN_EPOCHS,\n",
    "        \"params\": count_parameters(pixelcnn),\n",
    "        \"kid\": kid,\n",
    "        \"nll\": pixelcnn_nll,\n",
    "        \"sampling_time_s\": elapsed,\n",
    "        \"throughput_img_per_s\": throughput,\n",
    "        \"train_time_total_s\": pixelcnn_train_time,\n",
    "        \"train_time_per_epoch_s\": pixelcnn_train_time / PIXELCNN_EPOCHS,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Validation NLL: {pixelcnn_nll:.4f}\")\n",
    "print(f\"KID (x10^3): {kid:.3f}\")\n",
    "print(f\"Sampling time for {pixelcnn_samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.2f} img/s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e21e0a",
   "metadata": {},
   "source": [
    "> **Scaling experiment:** > **Scaling experiment:** Retrain the medium-scale configuration in below and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"PixelCNN\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99a441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "733efc6d",
   "metadata": {},
   "source": [
    "## DDPM (diffusion model)\n",
    "\n",
    "Verify the diffusion loss path in `models/ddpm.py` before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_cfg = EXPERIMENT_CONFIGS[\"ddpm\"][\"small\"]\n",
    "ddpm = DenoiseUNet(\n",
    "    image_channels=3,\n",
    "    base_channels=ddpm_cfg[\"base_channels\"],\n",
    "    time_channels=ddpm_cfg[\"time_channels\"],\n",
    "    timesteps=ddpm_cfg[\"timesteps\"],\n",
    ")\n",
    "optim_ddpm = torch.optim.Adam(ddpm.parameters(), lr=ddpm_cfg[\"lr\"])\n",
    "print(f\"Baseline DDPM parameters: {count_parameters(ddpm):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDPM_EPOCHS = ddpm_cfg[\"epochs\"]\n",
    "\n",
    "ddpm_history = train_ddpm(ddpm, train_loader, optimizer=optim_ddpm, epochs=DDPM_EPOCHS)\n",
    "ddpm_epoch_times = ddpm_history[\"epoch_time\"]  # populate this inside your implementation\n",
    "ddpm_train_time = sum(ddpm_epoch_times)\n",
    "print(f\"Finished DDPM training: {len(ddpm_history['loss'])} steps\")\n",
    "print(f\"DDPM training time per epoch (s): {[round(t, 2) for t in ddpm_epoch_times]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: once `sample_ddpm` is implemented, run the evaluation block below.\n",
    "ddpm_samples = sample_ddpm(ddpm, num_samples=64, device=DEVICE)\n",
    "ddpm_samples_vis = (ddpm_samples + 1.0) / 2.0\n",
    "save_image_grid(ddpm_samples_vis, ARTIFACT_DIR / \"ddpm_samples_baseline.png\", nrow=8)\n",
    "\n",
    "sample_budget = THROUGHPUT_SAMPLES[\"ddpm\"]\n",
    "samples_large, elapsed, throughput = measure_sampling_throughput(\n",
    "    lambda num_images, device: (sample_ddpm(ddpm, num_samples=num_images, device=device) + 1.0) / 2.0,\n",
    "    num_images=sample_budget,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "kid = compute_kid_score(real_subset_device[:samples_large.size(0)].cpu(), samples_large.cpu())\n",
    "record_metrics(\n",
    "    \"DDPM\",\n",
    "    \"baseline\",\n",
    "    {\n",
    "        \"epochs\": DDPM_EPOCHS,\n",
    "        \"params\": count_parameters(ddpm),\n",
    "        \"kid\": kid,\n",
    "        \"sampling_time_s\": elapsed,\n",
    "        \"throughput_img_per_s\": throughput,\n",
    "        \"train_time_total_s\": ddpm_train_time,\n",
    "        \"train_time_per_epoch_s\": ddpm_train_time / DDPM_EPOCHS,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"KID (x10^3): {kid:.3f}\")\n",
    "print(f\"Sampling time for {samples_large.size(0)} images: {elapsed:.2f} s (throughput {throughput:.2f} img/s)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7a276",
   "metadata": {},
   "source": [
    "> **Scaling experiment:** Retrain the medium-scale configuration in below and log metrics after 30 and 40 epochs (e.g. `record_metrics(\"DDPM\", \"scaled-30ep\", {...})` / `\"scaled-40ep\"`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfba670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a274362b",
   "metadata": {},
   "source": [
    "## Summary & export\n",
    "\n",
    "After running the experiments above (small and medium variants), use the helper below to view logged metrics and export them to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "show_metrics()\n",
    "\n",
    "rows = []\n",
    "for model_name, variants in experiment_log.items():\n",
    "    for variant, metrics in variants.items():\n",
    "        row = {\"model\": model_name, \"variant\": variant}\n",
    "        row.update(metrics)\n",
    "        rows.append(row)\n",
    "if rows:\n",
    "    df_metrics = pd.DataFrame(rows)\n",
    "    display(df_metrics.set_index([\"model\", \"variant\"]))\n",
    "\n",
    "summary_path = ARTIFACT_DIR / \"metrics_summary.json\"\n",
    "with summary_path.open(\"w\") as fp:\n",
    "    json.dump(experiment_log, fp, indent=2)\n",
    "print(f\"Saved metrics summary to {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c40f0f",
   "metadata": {},
   "source": [
    "## Aggregate plots\n",
    "\n",
    "Use this section to generate the plots and figures requested in the assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3787a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
